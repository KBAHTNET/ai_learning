# Репозиторий по изучению AI + pytorch

## Оглавление
1. [Линейная регрессия](!линейная-регрессия)
2. [Многослойные нейронне сети](!многослойные-нейронне-сети)
    1. Классификация
3. Функции потерь (criterion):
    1. CrossEntropyLoss
    2. MSELoss
4. Оптимизаторы (optimizer):
    1. SGD
    2. Momentum
    3. AdaGrad
    4. RMSprop
    5. Adam
5. Функции активации:
    1. ReLU
    2. Sigmoid

___

## Линейная и логистическая регрессия
Линейная регрессия и логистическая регрессия - это два основных метода машинного обучения, используемые для решения задачи предсказания.

**Линейная регрессия** - это метод, который используется для построения модели на основе линейной зависимости между входными признаками и целевой переменной. Она пытается найти линейную связь между входными признаками и выходными данными, которая может использоваться для прогнозирования новых значений. В линейной регрессии используется функция потерь, которая минимизирует разницу между предсказанными и фактическими значениями.

**Логистическая регрессия** - это метод машинного обучения, который используется для решения задач классификации. Она пытается найти линейную связь между входными признаками и вероятностью отнесения объекта к определенному классу. Логистическая регрессия использует сигмоидную функцию для приведения предсказанных значений к вероятностному диапазону от 0 до 1.

К другим подобным методам машинного обучения относятся:
- Решающие деревья: это метод, который используется для построения дерева решений, которое может использоваться для принятия решений на основе значений входных признаков.
- Случайный лес: это метод, который использует несколько решающих деревьев и усредняет результаты, чтобы получить более точную модель.
- Метод опорных векторов (SVM): это метод, который строит гиперплоскость, разделяющую объекты разных классов в многомерном пространстве.
- Нейронные сети: это метод, который моделирует работу человеческого мозга, используя слои нейронов для обработки данных и создания прогнозов.

Выбор метода машинного обучения зависит от ряда факторов, таких как тип задачи, количество доступных данных, качество данных и требуемая точность прогнозирования. Вот несколько рекомендаций по выбору метода машинного обучения для различных задач:

- Задачи регрессии: в случае, когда требуется предсказание числовой переменной, линейная регрессия может быть хорошим выбором, если имеются линейные зависимости между признаками и целевой переменной. Если данные содержат нелинейные зависимости, то можно использовать решающее дерево, случайный лес или нейронные сети.
- Задачи классификации: если требуется отнести объекты к определенным категориям, логистическая регрессия может быть хорошим начальным выбором. Если данные содержат нелинейные зависимости, можно использовать SVM или нейронные сети.
- Задачи кластеризации: если требуется группировать объекты на основе сходства, можно использовать алгоритмы кластеризации, такие как K-средних или DBSCAN.
- Обработка естественного языка: для анализа текстовых данных можно использовать методы машинного обучения, такие как LDA (Latent Dirichlet Allocation), Naive Bayes или SVM.
- Рекомендательные системы: для построения рекомендательных систем можно использовать методы коллаборативной фильтрации, контент-базированные методы или гибридные методы.
___

## Многослойные нейронне сети
123231


## Функции потерь
### CrossEntropyLoss
CrossEntropyLoss обычно используется для задач классификации, где требуется предсказать вероятности принадлежности объектов к различным классам. Она вычисляет расстояние между предсказанными вероятностями классов и фактическими метками классов. Функция потерь имеет вид:
```
loss(x, y) = -1/N * ∑[y_i*log(x_i) + (1-y_i)*log(1-x_i)]
```
где N - количество объектов в батче, x_i - предсказанная вероятность класса i, y_i - фактическая метка класса i (0 или 1).
### MSELoss
MSELoss используется для задач регрессии, где требуется предсказать числовое значение. Она вычисляет среднеквадратичную ошибку между предсказанными значениями и целевыми значениями. Функция потерь имеет вид:
```
loss(x, y) = 1/N * ∑[(x_i-y_i)^2]
```
где N - количество объектов в батче, x_i - предсказанное значение для объекта i, y_i - целевое значение для объекта i.

## Оптимизаторы

### SGD

SGD (Stochastic Gradient Descent) - это один из наиболее простых и широко используемых оптимизаторов в машинном обучении. Он используется для обновления весов модели на основе градиента функции потерь. Вот основные шаги, которые выполняет SGD:

1. Выбор случайного мини-пакета: SGD обновляет веса модели на основе градиента, оцененного на небольших случайно выбранных подмножествах данных, называемых мини-пакетами или батчами. Это позволяет снизить вычислительную сложность и ускорить процесс обучения.

2. Расчет градиента функции потерь: Для каждого мини-пакета SGD вычисляет градиент функции потерь по отношению к весам модели. Градиент показывает направление наискорейшего возрастания функции потерь и используется для обновления весов.

3. Обновление весов модели: После вычисления градиента SGD обновляет веса модели, двигая их в противоположном направлении градиента с определенным шагом, называемым скоростью обучения. Обычно обновление весов выполняется по формуле: новый_вес = старый_вес - скорость_обучения * градиент.

Лучшие аналоги SGD включают в себя более совершенные оптимизаторы, которые улучшают его производительность и скорость обучения. Некоторые из них включают:

- Momentum: Оптимизатор с моментом (Momentum) использует предыдущие градиенты для определения текущего направления обновления весов. Это позволяет ускорить обучение, особенно в случаях с большими и шумными градиентами.

- AdaGrad: Адаптивный градиент (AdaGrad) модифицирует скорость обучения для каждого веса в соответствии с их историческими градиентами. Это позволяет более агрессивно обновлять редко встречающиеся параметры и более консервативно обновлять часто встречающиеся параметры.

- RMSprop: RMSprop является вариантом AdaGrad, который сглаживает историю градиентов, чтобы предотвратить уменьшение скорости обучения в долгосрочной перспективе. Он улучшает сходимость и скорость обучения.

- Adam: Adam объединяет преимущества методов Momentum и RMSprop, комбинируя предыдущие градиенты с адаптивной скоростью обучения. Он широко используется в глубоком обучении и предлагает хороший баланс между эффективностью и скоростью обучения.

## Функции активации

### ReLU
- Функция ReLU определяется как f(x) = max(0, x). Она просто заменяет все отрицательные значения на ноль, а положительные значения оставляет без изменений.
- Преимущества ReLU включают простоту вычислений и отсутствие проблемы исчезающего градиента, которая может возникнуть при использовании других функций активации, например, сигмоиды.
- ReLU обычно работает хорошо в глубоких нейронных сетях с большим количеством скрытых слоев и способна обучать более разреженные представления данных.

Существуют несколько более сложных вариантов функции активации, которые можно использовать вместо ReLU. Некоторые из них включают:

1. Leaky ReLU: Leaky ReLU вводит небольшой наклон для отрицательных значений, вместо того чтобы обнулять их. Формула Leaky ReLU выглядит следующим образом:
f(x) = max(ax, x), где a - небольшое положительное число (обычно около 0.01).

2. Parametric ReLU (PReLU): PReLU является расширением Leaky ReLU, где наклон становится обучаемым параметром, а не фиксированным значением. Это позволяет модели адаптироваться к разным типам данных. Формула PReLU выглядит следующим образом:
f(x) = max(ax, x), где a - обучаемый параметр.

3. Exponential Linear Unit (ELU): ELU представляет собой экспоненциальную функцию для отрицательных значений и линейную функцию для положительных значений. Формула ELU выглядит следующим образом:
f(x) = x, если x >= 0; f(x) = a * (exp(x) - 1), если x < 0, где a - параметр, определяющий наклон в отрицательной области.

4. Swish: Swish представляет собой гладкую и нелинейную функцию, которая комбинирует элементы сигмоиды и ReLU. Формула Swish выглядит следующим образом:
f(x) = x * sigmoid(beta * x), где beta - обучаемый параметр.

### Sigmoid
- Функция сигмоиды определяется как f(x) = 1 / (1 + exp(-x)). Она сжимает входное значение в диапазоне [0, 1].
- Преимущества сигмоиды включают интерпретируемость вывода как вероятности и гладкость функции.
- Сигмоида может использоваться в задачах бинарной классификации, где нужно предсказать вероятность принадлежности к одному из двух классов.

### Softmax

Softmax - это функция активации, которая широко используется в многоклассовой классификации. Она преобразует вектор значений в вектор вероятностей, где каждый элемент выходного вектора представляет вероятность принадлежности к определенному классу.

Функция softmax определяется следующим образом:
```
softmax(x) = exp(x) / sum(exp(x))
```

Главное свойство softmax заключается в том, что она нормализует выходные значения таким образом, чтобы их сумма составляла 1. Это делает ее полезной для задачи классификации, где требуется предсказать вероятности принадлежности к различным классам.

Когда применяется softmax к выходному вектору модели, каждый элемент вектора интерпретируется как вероятность принадлежности соответствующему классу. Класс с наибольшей вероятностью выбирается как предсказанный класс.

Пример использования softmax:
```py
output = [2.0, 1.0, 0.5]
softmax_output = softmax(output)
print(softmax_output)
```
Результат:
```py
[0.66524096, 0.24472847, 0.09003057]
```
Здесь softmax преобразовал исходный выходной вектор в вектор вероятностей, где первый элемент имеет наибольшую вероятность.